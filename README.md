# Multi-Modal, Multi-Lingual Stance Detection

This repository contains code for running experiments using various Vision-Language Models
on the stance detection dataset from [Liang et al., 2024](https://arxiv.org/abs/2402.14298).

## Requirements

 - Python >=3.9
 - `pytorch`
 - `transformers`
 - `numpy`
 - `pandas`
 - `scikit-learn`
 - `tqdm`
 - `pillow`
 - `statsmodels`
 - `matplotlib`
 - `seaborn`
 - `qwen_vl_utils`
 - [experiment\_config](https://github.com/jvasilakes/experiment-config)


## Obtaining the Data


The dataset is freely available at https://github.com/Leon-Francis/Multi-Modal-Stance-Detection/.
However, you'll need to hydrate the tweets and download the images yourself.
 
  

## Running Experiments

To run a single 0-shot prediction experiment, simply call the following

```
python run.py predict --split {datasplit} {config_file}
```

where `{datasplit}` is one of `train, validation, test` and `{config_file}`
is a configuration file detailing the experiment. An explanation of how to create
and work with config files is given below.


## Experiment Config Files

Config file management is done using the [experiment\_config](https://github.com/jvasilakes/experiment-config) library.
After installing it, a default config file can be generated by running

```
python config.py new myconfig.yaml
```

Then fill in the values according to the comments included for each variable.


`experiment_config` provides a number of tools for modifying config files easily from the command line.
Check out the documentation to learn more.


### Evaluation

The predictions resulting from `run.py predict` will be saved to `{Experiment.output_dir}/predictions/{datasplit}.csv`.
Here `{Experiment.output_dir}` is specified by the config file. To compute precision, recall, and F1 score of these
predictions run

```
python scripts/evaluate.py {Experiment.output_dir}/predictions/{datasplit}.csv
```

A JSON file of the results will be automatically saved to `{Experiment.output_dir}/predictions/results/{datasplit}.json`

If you want average results over 1 or more prediction files, simply specify all of the files as arguments to
`scripts/evaluate.py`. In this case the result will be printed to the terminal.
